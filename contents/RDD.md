# RDD 概念

Spark 的一切都是建立在RDD这个抽象概念之上，而且也正是因为RDD的设计理念让Spark能比Hadoop更方便的实现兼容各类数据集。学习Spark之前，必须要对RDD有所了解，所以我们下面简单介绍下RDD。

1. RDD是什么？
    
    RDD的全称为Resilient Distributed Datasets，是被设计为具有分布式、容错能力的只读数据结构。
2. RDD为何是分布式
    
    每个RDD实际是将一个数据集分为了多个分区的小数据集存储在集群的机器上。由于RDD是只读，所以对RDD操作处理都会生成新的RDD，这避免了对同一份数据进行不同更新处理操作带来的问题。

    实际来说，RDD在分布式方面的设计理念并不是新颖而复杂的概念，而就是现今在函数式并行设计的数据只读概念 和 分布式存储概念。
    
3. RDD为何具有容错能力

    RDD在设计之时为了解决在Hadoop Map Reduce 中不具有容错机制，引入了依赖（可以理解为“血源”或者“族谱”之类的东东）的概念。
    
    简单来说这个概念就是RDD会保存自身是从那些RDD以及如何从这些RDD生成的记录，当我们计算一个新的RDD时，发现它的RDD源中有一个或者多个RDD由于GC、数据异常丢失等不存在了，但是我们知道这些不能使用的RDD是如何生成的操作记录，我们里面可以重新按照对应操作记录生成RDD。这样就不会像在Hadoop Map Reduce中，我们不得不从头开始重新计算全部数据。
    
    假如我们正常操作如图所示：
    ![正常操作](RDD_Normal.png)
    
    当假如我们在操作 2 时，发现RDD B 丢失，我们就可以直接从RDD A 进行 操作 1 而生成RDD B，接着我们就可以继续进行操作 2了。
    
4. RDD 的两种不同依赖概念

    RDD所引入的依赖概念实际分为两种：窄依赖和宽依赖。
    
    1) 窄依赖 简单来说就是RDD B 只是通过 RDD A 操作而得到，这种一对一的方式就是窄依赖。
    
    2) 宽依赖 简单来说就是RDD E 必须通过 RDD C 和 RDD D进行join等操作处理才能得到，这中一对多的方式就是宽依赖。
    
    为什么要区分这两种依赖方式呢?感觉无非是一个生成一个，多个生成一个而已？
    
    实际区分的原因是分布式计算的设计理念：为了更好计算数据，如果一个节点只计算自身节点上的数据，不用计算时必须从别的节点获取数据，这计算性能将得到极大提升。当然实际应用中，我们是不可能做到这一点的，但是当我们必须做汇总之类的处理时，我们拿到的多个节点的数据是已经每个节点做了自身节点能做的所有处理之后的数据，那么我们汇总所有数据时的量或复杂操作将有一定程度的减小。
    
    回到两个不同的依赖概念上：
    
    A) 窄依赖 由于只是从一个RDD生成另一个RDD，那么我们就可以完全做到一个节点只计算自身节点的数据，数据丢失时我们可以只从自身节点就可以恢复数据了。
    
    B) 宽依赖 无法做到窄依赖那样，所以我们不得不做一些窄依赖没有处理才能生成或恢复RDD。
    
    这就是为什么RDD要重点区分出这两种依赖的原因。
5. 


